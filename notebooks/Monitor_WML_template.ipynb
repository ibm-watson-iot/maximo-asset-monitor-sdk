{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train** a ML model locally, **Deploy** the model in Watson Studio, and perform real-time **Prediction** in Maximo-Monitor\n",
    "\n",
    "\n",
    "You can train and deploy a machine learning model to Watson Studio in several ways. You can use this notebook as template for training your model locally using data from Maximo Monitor. Then, deploy your model to Watson Studio. At the end of this notebook, you'll find more information about how to get started with real-time inferencing.\n",
    "\n",
    "\n",
    "If you are new to Watson Machine Learning and Cloud Pak for Data, see the documentation at the following links to learn more:\n",
    "- [Watson Machine Learning Documentation](https://www.ibm.com/docs/en/cloud-paks/cp-data/3.5.0?topic=deploying-managing-models-functions)\n",
    "- [Jupyter Notebook examples for deploying and using Watson Studio models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-samples-overview.html?context=cpdaas&audience=wdp)\n",
    "- This notebook is based on [Watson Studio example notebook](https://github.com/IBM/watson-machine-learning-samples/blob/master/cpd4.0/notebooks/python_sdk/deployments/custom_library/Use%20scikitlearn%20and%20custom%20library%20to%20predict%20temperature.ipynb)\n",
    "\n",
    "\n",
    "**Note** The following files are required to run this notebook and they are not provided with this template: \n",
    "1. monitor-credentials.json. Contains credentials to connect to maximo monitor. Contents explained in section **I. Credentials setup**\n",
    "2. wml-credentials.json. Contains credentials to connect to watson machine learning API. Contents explained in section **I. Credentials setup**\n",
    "\n",
    "Create your own files with the correct file paths. In the template, all required inputs are marked with the `user-input-required` comment as a guide.\n",
    "\n",
    "**Pre-requisites**\n",
    "1. Install the iotfunctions package\n",
    "2. Install any other packages that are required to train specific machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient # watson studio\n",
    "import json # loading credentials\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Credentials setup\n",
    "You must create two sets of credentials:\n",
    "\n",
    "1. Credentials to allow Maximo Monitor access its database and APIs.\n",
    "2. Credentials to allow Watson Machine Learning to access Watson Studio deployment spaces. \n",
    "\n",
    "The credentials are saved in JSON files. In the following cell, define the relative path (that is, relative to this notebook) of both of these credentials files and load the files from local variables\n",
    "\n",
    "**Note**: Keep all credentials safe and hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONITOR_CREDENTIALS_FILE_PATH = './dev_resources/monitor-credentials.json'  # user-input-required. Set it the path of your monitor-credentials file relative to this notebook\n",
    "WML_CREDENTIALS_FILE_PATH = './dev_resources/wml-credentials.json' # user-input-required. Set it the path of your wml-credentials file relative to this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More information about credentials in Maximo Monitor**\n",
    "\n",
    "[Maximo Monitor documentation](https://www.ibm.com/docs/en/maximo-monitor/8.5.0?topic=monitor-connection-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MONITOR_CREDENTIALS_FILE_PATH, 'r') as f: \n",
    "    monitor_credentials = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More information about credentials in Watson Machine Learning**\n",
    "\n",
    "1. [Generate new API key](https://cloud.ibm.com/iam/apikeys) for Watson Machine Learning services or use an existing API key\n",
    "\n",
    "2. Save your Watson Machine Learning credentials to a JSON file\n",
    "\n",
    "```json\n",
    "{\n",
    "    url:\"https://us-south.ml.cloud.ibm.com\",\n",
    "    apikey:\"xxxx\"\n",
    "}\n",
    "```\n",
    "**Note**: Depending on the region of your provisioned service instance, use one of the following as your url:\n",
    "```\n",
    "Dallas: https://us-south.ml.cloud.ibm.com\n",
    "London: https://eu-gb.ml.cloud.ibm.com\n",
    "Frankfurt: https://eu-de.ml.cloud.ibm.com\n",
    "Tokyo: https://jp-tok.ml.cloud.ibm.com\n",
    "```\n",
    "\n",
    "[More information](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html?context=cpdaas&audience=wdp) about WML Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WML_CREDENTIALS_FILE_PATH, 'r') as f: \n",
    "    wml_credentials = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Connect to Watson Machine Learning API Client\n",
    "\n",
    "Connect to the API Client using the credentials that you loaded in the previous step. This client is used to \n",
    " 1. Extract deployment spaces\n",
    " 2. Set the deployment space to the space you want to deploy our model\n",
    " 3. Extract the software specification that you will use to deploy the model\n",
    " 4. Extract the hardware specification that you will use to deploy the model\n",
    " \n",
    "[Watson Machine Learning API Client Documentation](http://ibm-wml-api-pyclient.mybluemix.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Space\n",
    "\n",
    "To deploy a model to Watson Machine Learning services, you must connect to a deployment space:\n",
    "\n",
    "1. [Create a deployment space](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-spaces_local.html?context=cpdaas&audience=wdp) if you don't have one yet\n",
    "2. Set the space ID manually using the list generated from `client.spaces.list()`\n",
    "\n",
    "Note: if `client.spaces.list()` is empty you will need to create a deployment space as specified in Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.spaces.list(limit=10) #lists deployment spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set.default_space('select-and-set-space-id') # user-input-required. Select a space id from the list generated in the above cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Software Specification**\n",
    "\n",
    "[Learn more about software specification and why you need them](https://www.ibm.com/docs/en/cloud-paks/cp-data/3.5.0?topic=overview-specifying-model-type-software-specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.software_specifications.list(limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardware Specification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware specifications determine how much CPUs and RAM can be used during inferencing. Different specifications allow you to scale the deployment as needed (but at a cost).\n",
    "\n",
    "[Learn more about CUH costs for different hardware specifications](https://dataplatform.cloud.ibm.com/docs/content/wsj/landings/wml-plans.html?context=cpdaas&audience=wdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.hardware_specifications.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Train Model\n",
    "\n",
    "[Read the article about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) if you are interested in getting familiar with some of the basics.\n",
    "\n",
    "This notebook can be modified and used for regression or classifiaction tasks. [A short refresher for classification vs regression tasks](https://in.springboard.com/blog/regression-vs-classification-in-machine-learning/)\n",
    " \n",
    "The template loads an entity's data and guides you to train a machine model locally by using that data. An entity in Monitor has several database tables that store metrics, dimensions, derived metrics, and alerts. To load these features for an entity, follow this step-by-step guide.\n",
    "\n",
    "**Steps to train a model in a notebook**\n",
    "1. Load training data from Monitor database (for a specified entity)\n",
    "2. Prepare dataset for training\n",
    "3. Create a training pipeline\n",
    "4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1 Load training data from Monitor database**\n",
    "\n",
    "An entity in Monitor has several database tables that store raw metrics, dimensions, and alerts. To load these features you peform two database reads. The first read fetches raw metrics and dimensions, and the second read fetched the alerts for the user provided entity type. Both data streams are stored is separate comma separated value (.csv) files and are available to manipulate as required.\n",
    "\n",
    "\n",
    "Complete the following these steps to load training data:\n",
    "1. Connect to the database\n",
    "2. Specify entity_name (and any other user input)\n",
    "3. Save the data in a csv to use later\n",
    "\n",
    "**Note** If you already have data available in a csv file, go to section **III.2 Prepare data\n",
    "\n",
    "Set the following variables:\n",
    "- (Required) entity_name  # Name of the entity that you want to retrieve data from. Use the same name that is displayed on the user interface in Monitor.,\n",
    "- (Optional) start_ts # Start fetching date from this date and time.,\n",
    "- (Optional) end_ts # Fetch data until this date and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.1 Connect to the database**\n",
    "\n",
    "Use the monitor credentials loaded section **I. Credentials setup** to connect to the Monitor database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iotfunctions.db import Database\n",
    "\n",
    "db = Database(credentials = monitor_credentials)\n",
    "db_schema = None #  set if you are not using the default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.2 Specify entity_name (and any other user input)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (User Input) user-input-required\n",
    "\n",
    "entity_name = 'shraddha_robot' # user-input-required. Set to entity name you'd like to retrieve data from. This is the same name as displayed on the Monitor UI\n",
    "\n",
    "start_ts = None # user-input-required. (Optional) Fetch data starting from this date/time. Format 'YYYY-MM-DD-HH.MM.ss.mmmmm'. Set to None to disable\n",
    "\n",
    "end_ts = None # user-input-required. (Optional) Fetch data until this date/time. Format 'YYYY-MM-DD-HH.MM.ss.mmmmm'. Set to None to disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = db.get_entity_type(name=entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = entity_type.get_data(start_ts = start_ts, end_ts=end_ts) # fetch metric and dimension data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loaded data: metris and dimension**\n",
    "\n",
    "The metrics and dimension data for the entity type you specified is loaded in a dataframe that is indexed with a (id, evt_timestamp) index. `id` is the device ID and `evt_timestamp` is the event time. This is the same index scheme that the pipeline uses on its dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Setup to fetch alerts)\n",
    "alert_table_name = \"DM_WIOT_AS_ALERT\"\n",
    "alert_table_timestamp_col = \"timestamp\"\n",
    "alert_filters = {\"entity_type_id\": [entity_type._entity_type_id]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, _ = db.query(table_name=alert_table_name, schema=db_schema,timestamp_col=alert_table_timestamp_col, start_ts=start_ts, end_ts=end_ts, filters=alert_filters) #query for alert data\n",
    "alert_df = db.read_sql_query(sql=query.statement) # fetch alert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loaded data: alerts**\n",
    "\n",
    "Alert data for the specified entity type is loaded in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can perform further data evaluations and manipulations in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.3 Save data in csv**\n",
    "\n",
    "The metric (with dimensions) and alert data that was loaded in the previous steps can now be saved into .csv files. Running the following step saves this data into files named **metric_{{entity_name}}.csv** and **alert_{{entity_name}}.csv** respectively. In your environment, `{{entity_name}}` is replaced by the name you specified during the load data stage.\n",
    "\n",
    "\n",
    "Both files are saved in the same directory as this notebook. You can create a separate directory for the data by appending the relative directory structure to the following file path. For example, to save the data to a folder named `data` under the directory that contains this notebook, modify the file path as follows:\n",
    "```python3\n",
    "metric_data_filepath = f'./data/metric_{entity_name}.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_data_filepath = f'metric_{entity_name}.csv'\n",
    "alert_data_filepath = f'alert_{entity_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.to_csv(metric_data_filepath, index=False,header=True) # save metric and dimension data\n",
    "alert_df.to_csv(alert_data_filepath, index=False,header=True) # save alert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.2 Prepare data**\n",
    "\n",
    "Load the data that you saved earlier and manipulates it some more to prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data back**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df = pd.read_csv(metric_data_filepath) # assumes you saved data using steps above. If you have pre-save data replace `metric_data_filepath` with the path to that .csv. The path should be relative to this notebook \n",
    "raw_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_data_df = pd.read_csv(alert_data_filepath) # assumes you saved data using steps above. If you have pre-save data replace `alert_data` with the path to that .csv. The path should be relative to this notebook \n",
    "alert_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataset to train the model**\n",
    "\n",
    "This template uses the variables `speed` and `acc` to predict `torque`. Depending on your problem statement, you can use all or part of the available data to define your feature vector as well as the prediction variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = ['acc', 'speed'] # use-input-required. Define feature vector. These are the variables used to train the model\n",
    "target_variable = 'torque' # use-input-required. Define target variable. This is the feature you want to predict. For regression models this will be a continous variable and for classification models this is a discrete variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_data_items = []\n",
    "required_data_items.extend(feature_vector)\n",
    "required_data_items.append(target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = raw_data_df[required_data_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further data manipulation can be performed in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = data_df['torque'] # user-input-required. Specify the target/prediction/classification variable here \n",
    "X = data_df[['acc', 'speed']] # user-input-required. Provide your feature vector here\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=143) # create a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cell for train, test data verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.3 Create training pipeline**\n",
    "\n",
    "In this template a sample training pipeline is provided. The training pipeline uses [sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that imlpements data transformation steps followed by an estimator (the model). Using the pipeline is optional.\n",
    "\n",
    "Learning resources\n",
    "- [Sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- [Sklearn Transformers for text data](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)\n",
    "- [Sklearn Feature Selection](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html?highlight=selectfrommodel#sklearn.feature_selection.SelectFromModel)\n",
    "- [Sklearn Hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search)\n",
    "- [Python packages for machine learning](https://github.com/ml-tooling/best-of-ml-python#time-series-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code builds a preprocessor to transform input data. The numeric and categorical data are transformed using separate methods. In the sample code, you'll find a pipeline with a imputer (to complete missing data) and a scaler to transform numerical data. Using the code, the categorical features are transformed into one hot encoded vector. Both of these steps are combined to build the `preprocessor` module to perform input data transformation.\n",
    "\n",
    "Use the sklearn learning resources to learn more about the individual parts of the `ColumnTransformer` module and to learn about the different options for data imputers, scalers, and text data transformers that are available.\n",
    "\n",
    "\n",
    "```python3\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "    \n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "```\n",
    "\n",
    "`numeric_features` is a list of all numerical feature names. In our example `numerical_features=['acc', 'speed']` <br>\n",
    "`categorical_features` is a list of all numerical feature names\n",
    "\n",
    "\n",
    "To create and use relevant features you can use `SelecFromModel` for an additional `feature_selection` stage in the training pipeline\n",
    "\n",
    "```python3\n",
    "Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                 ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "                 ('classification', LinearRegression())])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = list(set(X.columns.tolist()).difference(numeric_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "## Example Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "skl_pipeline = Pipeline(steps=[('preprocessor', preprocessor), # (Optional) preprocessor stage\n",
    "                               ('classification', GradientBoostingRegressor())]) # user-input-required estimator/machine learnign model you want to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.4 Train model**\n",
    "\n",
    "You can run the training pipeline by completing the setup in the previous cell or you can set up hyperparameter tuning as follows and run the search pipeline. The training pipeline runs a single training job while the hyperparameter search runs several jobs to find the best model parameters across the ranges specified in `param_grid`. The search pipeline is more computationally intensive and so it takes longer to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Set up hyper paratmeter tuning**\n",
    "\n",
    "- Parameters of pipelines can be set using `__` separated parameter names\n",
    "- Specify parameters to train in `param_grid`\n",
    "- Specify other parameters as needed\n",
    "\n",
    "Refer to [Tuning hyperparameter](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search) for further details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "use_hyperparameter_tuning = True\n",
    "use_hyperparameter_tuning = True \n",
    "\n",
    "param_grid = {\n",
    "    'classification__loss': ['ls', 'lad']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(skl_pipeline, param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Training Job**\n",
    "\n",
    "Depending on factors such as the model you selected, whether you are running hyperparameter training, and the computational power you have available to run this notebook, the following cell might take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'use_hyperparameter_tuning' in globals() and use_hyperparameter_tuning:\n",
    "    print('Running the Hyperparameter tuning ...')\n",
    "    search.fit(X_train, y_train)\n",
    "    model = search\n",
    "else:\n",
    "    print('Running training pipeline ...')\n",
    "    model = skl_pipeline\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('Finished training job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run additional validation on model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "rmse = np.mean((np.round(y_pred) - y_test.values)**2)**0.5\n",
    "print('RMSE: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Save a local copy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "model_filename = 'monitor_wml_model.sav'\n",
    "pickle.dump(model, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Load the local copy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(model_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Deploy the Model in Watson Machine Learning\n",
    "\n",
    "In this step, you use the connection to WML python client that you made in **Step II. Connect to Watson Machine Learnig API Client**. When you deploy the trained model, you must **a.** set the metadata, **b.** retain the model and, **c.** deploy the model to Watson Machine Learning.,\n",
    "\n",
    "For **a.**, set the following metadata,\n",
    "1. Model name\n",
    "2. Software specification. Run the following command to view a list of software specification options: `client.software_specifications.list()`. Pick the specification best suited for your model.\n",
    "3. Hardware specification Run the following command to view a list of hardware specification options: `client.hardware_specifications.list()`. Pick the specification best suited for your model.\n",
    "\n",
    "(These specifications were extracted when you connected to the Watson Studio Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sw_spec_uid = client.software_specifications.get_uid_by_name(\"default_py3.7\") # user-input-required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sets up the software specifications when using only scikit-learn for modeling purposes. For information about more intricate software setup when using custom packages see the [Watson ML sample notebooks](https://www.ibm.com/docs/en/cloud-paks/cp-data/3.5.0?topic=apis-machine-learning-python-example-notebooks) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_props = {\n",
    "    client.repository.ModelMetaNames.NAME: \"MaxTemp prediction model\",\n",
    "    client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23', # user-input-required\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: base_sw_spec_uid\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Retain model**\n",
    "\n",
    "To save the trained model, specify a software specification. Learn more about the available [software specifications](https://dataplatform.cloud.ibm.com/docs/content/wsj/wmls/wmls-deploy-python-types.html?context=cpdaas&audience=wdp) options and their usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model = client.repository.store_model(model=model, meta_props=model_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Deploy model**\n",
    "\n",
    "For this deployment, a hardware specification that requires the least amount of CUHs is used. The decision to use a different hardware specification can be guided by speed, performance and other variables. <br>\n",
    "[More Information on CUH usage for different hardware specifications](https://dataplatform.cloud.ibm.com/docs/content/wsj/landings/wml-plans.html?context=cpdaas&audience=wdp)\n",
    "\n",
    "\n",
    "The default hardware specification is S(2CPUs 16 GB). In this template, the specification is changed to XXS(1CPU 8 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of test model\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "    client.deployments.ConfigurationMetaNames.HARDWARE_SPEC : { \"id\":  \"b128f957-581d-46d0-95b6-8af5cd5be580\"} # user-input-required\n",
    "}\n",
    "\n",
    "created_deployment = client.deployments.create(published_model_uid, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract deployment uuid and other information**\n",
    "\n",
    "The deployment uuid is required to make predictions.\n",
    "\n",
    "Set the **wml_auth** constant field as defined in [Maximo Monitor documentation](https://www.ibm.com/docs/en/maximo-monitor/8.6.0?topic=detectors-using-externall-model). This field is used by the `InvokeWatsonStudio` catalog function to make real-time predictions by using the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_uid = client.deployments.get_uid(created_deployment)\n",
    "print(deployment_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.get_details(deployment_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Information needed for scoring\n",
    "1. wml_credentials\n",
    "2. Deployment space_id\n",
    "3. Deployment ID\n",
    "4. Feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) V. Test predictions using the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_endpoint = client.deployments.get_scoring_href(created_deployment)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_payload = {\n",
    "    \"input_data\": [{\n",
    "        'fields': ['acc', 'speed'],\n",
    "        'values': [[22, 23]]}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.deployments.score(deployment_uid, scoring_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, you have deployed a trained model by using Watson Machine Learning services. Congratulations!,\n",
    "\n",
    "Use the catalog function **InvokeWatsonStudio**, as described in the [Maximo Monitor documentation](https://www.ibm.com/docs/en/maximo-monitor/8.6.0?topic=detectors-using-externall-model), to make predictions from the Monitor pipeline by using the deployed model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
