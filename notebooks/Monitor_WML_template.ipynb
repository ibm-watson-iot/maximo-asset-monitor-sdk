{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train** a ML model locally, **Deploy** the model in Watson Studio, and perform real-time **Prediction** in Maximo-Monitor\n",
    "\n",
    "\n",
    "A ML model can be trained and deployed to Watson Studio in several different ways. This notebook is a template for training a model locally using data in Maximo-Monitor, and deploying the model in Watson Studio. At the end of this notebook there is further instructions on how to get started with real-time inference.\n",
    "\n",
    "\n",
    "If you are new to Watson Machine Learning/Cloud Pak for Data you can refer to these links to learn more\n",
    "- [Watson Machine Learning Documentation](https://www.ibm.com/docs/en/cloud-paks/cp-data/3.5.0?topic=deploying-managing-models-functions)\n",
    "- [Jupyter Notebook examples for deploying and using Watson Studio models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-samples-overview.html?context=cpdaas&audience=wdp)\n",
    "- This notebook is based on [Watson Studio example notebook](https://github.com/IBM/watson-machine-learning-samples/blob/master/cpd4.0/notebooks/python_sdk/deployments/custom_library/Use%20scikitlearn%20and%20custom%20library%20to%20predict%20temperature.ipynb)\n",
    "\n",
    "\n",
    "**Note** Files required to run this notebook that are not provided in this template are: \n",
    "1. monitor-credentials.json. Contains credentials to connect to maximo monitor. Contents explained in section **I. Credentials setup**\n",
    "2. wml-credentials.json. Contains credentials to connect to watson machine learning API. Contents explained in section **I. Credentials setup**\n",
    "3. Weather.csv. Contains sample data for training. Contents explained in section **III. Training**\n",
    "\n",
    "You need to provide your own files with the correct file paths. In the template all required inputs are marked with `user-input-required` comment as a guide to fill in the required fields.\n",
    "\n",
    "**Pre-requisites**\n",
    "1. install iotfunctions\n",
    "2. install any other package/s required to train sepcific ml models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient # watson studio\n",
    "import json # loading credentials\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Credentials setup\n",
    "You need two sets of credentials\n",
    "1. credentials for monitor to access maximo-monitor database and monitor apis\n",
    "2. credentials for wml to access watson studio deployment spaces \n",
    "\n",
    "The credentials are saved in json files. In the cell below define the relative path (relative to this notebook) for both these credentials file and load the files in local variables\n",
    "\n",
    "**Note**: Keep the credentials safe and hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONITOR_CREDENTIALS_FILE_PATH = './dev_resources/monitor-credentials.json'  \n",
    "WML_CREDENTIALS_FILE_PATH = './dev_resources/wml-credentials.json' # user-input-required. Set it the path of your wml-credentials file relative to this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More information for monitor-credentials**\n",
    "\n",
    "[Monitor documentation](https://www.ibm.com/docs/en/maximo-monitor/8.5.0?topic=monitor-connection-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MONITOR_CREDENTIALS_FILE_PATH, 'r') as f: \n",
    "    monitor_credentials = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More information for wml-credentials**\n",
    "\n",
    "1. [Generate new api key](https://cloud.ibm.com/iam/apikeys) for Watson Machine Learning Services or use a api key generated before\n",
    "\n",
    "2. Save wml-credentials in a json file\n",
    "\n",
    "```json\n",
    "{\n",
    "    url:\"https://us-south.ml.cloud.ibm.com\",\n",
    "    apikey:\"xxxx\"\n",
    "}\n",
    "```\n",
    "**Note**: Depending on the region of your provisioned service instance, use one of the following as your url:\n",
    "```\n",
    "Dallas: https://us-south.ml.cloud.ibm.com\n",
    "London: https://eu-gb.ml.cloud.ibm.com\n",
    "Frankfurt: https://eu-de.ml.cloud.ibm.com\n",
    "Tokyo: https://jp-tok.ml.cloud.ibm.com\n",
    "```\n",
    "\n",
    "[More information](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html?context=cpdaas&audience=wdp) about WML Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WML_CREDENTIALS_FILE_PATH, 'r') as f: \n",
    "    wml_credentials = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Connect to Watson Machine Learning APIClient\n",
    "\n",
    "Connect to the APIClient using the credentials loaded in the previous step. This client is used to \n",
    " 1. extract deployment spaces\n",
    " 2. set the deployment space to the space you want to deploy our model in\n",
    " 3. extract the software specification that you will use to deploy the model\n",
    " 4. extract the hardware specification that you will use to deploy the model\n",
    " \n",
    "[WML API Client Documentation](http://ibm-wml-api-pyclient.mybluemix.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Space\n",
    "\n",
    "To deploy a model in WML services you need to connect to a deployment space\n",
    "\n",
    "1. [Create a deployment space](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-spaces_local.html?context=cpdaas&audience=wdp) if you don't have one yet\n",
    "2. Set the space id manually using the list generated from `client.spaces.list()`\n",
    "\n",
    "Note: if `client.spaces.list()` is empty you will need to create a deployment space as specified in Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.spaces.list(limit=10) #lists deployment spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set.default_space('select-and-set-space-id') # user-input-required. Select a space id from the list generated in the above cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Software Specification**\n",
    "\n",
    "[Learn more about spftware specification and why we need them](https://www.ibm.com/docs/en/cloud-paks/cp-data/3.5.0?topic=overview-specifying-model-type-software-specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.software_specifications.list(limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardware Specification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware specifications determine the CPUs and RAM used during inference. Different specifications allows you to scale the deploylment as needed (as a cost).\n",
    "\n",
    "[Learn more about CUH costs for different hardware specifications](https://dataplatform.cloud.ibm.com/docs/content/wsj/landings/wml-plans.html?context=cpdaas&audience=wdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.hardware_specifications.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Train Model\n",
    "\n",
    "[Checkout an article about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) if you are interested in getting familiar with some basics.\n",
    "\n",
    "This notebook can be modified and used for regression or classifiaction tasks. [A short refresher for classification vs regression tasks](https://in.springboard.com/blog/regression-vs-classification-in-machine-learning/)\n",
    "\n",
    "The template provided will load an entity's data and guide you to train a ML model locally using that data. An entity in Monitor has several database tables that store metrics, dimensions, derived metrics, and alerts. To load these several features we provide a step by step guide.  \n",
    "\n",
    "**Steps to train a model in a notebook**\n",
    "1. Load training data from Monitor database (for a specified entity)\n",
    "2. Prepare dataset for training\n",
    "3. Create a training pipeline\n",
    "4. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1 Load training data from Monitor database**\n",
    "\n",
    "An entity in Monitor has several database tables that store raw metrics, dimensions, and alerts. To load these several features we will peform two database reads. The first read will fetch raw_metrics and dimensions, and the second read will fetch the alerts for the user provided entity type. Both these data streams are store is separate comma-separate-values (.csv) files, and avaiable to manipulate as per your needs\n",
    "\n",
    "\n",
    "The cells below follow these steps to load training data\n",
    "1. Connect to the database\n",
    "2. Specify entity_name (and any other user input)\n",
    "3. Save the data in a csv to use later\n",
    "\n",
    "**Note** If you already have data available in acsv file you can skip this section **III.1 Load training data from Monitor database** and jump directly to section **III.2 Prepare data**\n",
    "\n",
    "The user needs to set the following variable in the cell below:\n",
    "- (Required) entity_name  # name of entity name you'd like to retrieve data from. This is the same name as displayed on the Monitor UI\n",
    "- (Optional) start_ts # Fetch data starting from this date/time\n",
    "- (Optional) end_ts # Fetch data until this date/time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.1 Connect to the database**\n",
    "\n",
    "Use the monitor credentials loaded section **I. Credentials setup** to connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iotfunctions.db import Database\n",
    "\n",
    "db = Database(credentials = monitor_credentials)\n",
    "db_schema = None #  set if you are not using the default\n",
    "\n",
    "entity_type = db.get_entity_type(name=entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Setup to fetch alerts)\n",
    "alert_table_name = \"DM_WIOT_AS_ALERT\"\n",
    "alert_table_timestamp_col = \"timestamp\"\n",
    "alert_filters = {\"entity_type_id\": [entity_type._entity_type_id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.2 Specify entity_name (and any other user input)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (User Input) user-input-required\n",
    "\n",
    "entity_name = 'shraddha_robot' # user-input-required. Set to entity name you'd like to retrieve data from. This is the same name as displayed on the Monitor UI\n",
    "\n",
    "start_ts = None # user-input-required. (Optional) Fetch data starting from this date/time. Format 'YYYY-MM-DD-HH.MM.ss.mmmmm'. Set to None to disable\n",
    "\n",
    "end_ts = None # user-input-required. (Optional) Fetch data until this date/time. Format 'YYYY-MM-DD-HH.MM.ss.mmmmm'. Set to None to disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = entity_type.get_data(start_ts = start_ts, end_ts=end_ts) # fetch metric and dimension data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loaded data: metris and dimension**\n",
    "\n",
    "The metrics and dimension data for thhe given entity type is loaded in a dataframe indexed with a (id, evt_timestamp) index. `id` is the deviceid and `evt_timestamp` is the event time. This is the same index scheme that the pipeline uses on it's dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, _ = db.query(table_name=alert_table_name, schema=db_schema,timestamp_col=alert_table_timestamp_col, start_ts=start_ts, end_ts=end_ts, filters=alert_filters) #query for alert data\n",
    "alert_df = db.read_sql_query(sql=query.statement) # fetch alert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loaded data: alerts**\n",
    "\n",
    "Alerts data for the provided entity type is loaded in a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can perform further data evaluations and manipulations in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.3 Save data in csv**\n",
    "\n",
    "The metrics+dimension and alerts data loaded in previous steps can now be saved into .csv files. Running the cell below will save these into files named **metric_{{entity_name}}.csv**  **alert_{{entity_name}}.csv** respectively. In your environment `{{entity_name}}` is replaces by the user specified value in the load data stage.\n",
    "\n",
    "Both files are saved in the same directory as this notebook. If you wish to create a separate directory to save data you can do so by apeending the relative directory structure to the filepath specified below. For example, if you place the data in a folder named `data` under the directory that contains this notebook modify the filepath like this: \n",
    "```python3\n",
    "metric_data_filepath = f'./data/metric_{entity_name}.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_data_filepath = f'metric_{entity_name}.csv'\n",
    "alert_data_filepath = f'alert_{entity_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.to_csv(metric_data_filepath, index=False,header=True) # save metric and dimension data\n",
    "alert_df.to_csv(alert_data_filepath, index=False,header=True) # save alert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.2 Prepare data**\n",
    "\n",
    "Load the data saved earlier and manipulate it further to prepare it for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data back**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df = pd.read_csv(metric_data_filepath) # assumes you saved data using steps above. If you have pre-save data replace `metric_data_filepath` with the path to that .csv. The path should be relative to this notebook \n",
    "raw_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_data_df = pd.read_csv(alert_data_filepath) # assumes you saved data using steps above. If you have pre-save data replace `alert_data` with the path to that .csv. The path should be relative to this notebook \n",
    "alert_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataset that will be used to train the model**\n",
    "\n",
    "In this template we will use variables `speed` and `acc` to predict `torque`. Depending on your problem statement you can use the all or part of the avaiable data to define your feature vector as well as the prediction variable/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = raw_data_df[['acc', 'speed', 'torque']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further data manipulation can be performed in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = data_df['torque'] # user-input-required. Specify the target/prediction/classification variable here \n",
    "X = data_df[['acc', 'speed']] # user-input-required. Provide your feature vector here\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=143) # create a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cell for train, test data verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.3 Create training pipeline**\n",
    "\n",
    "In this template a sample training pipeline is provided. The training pipeline uses [sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that imlpements data transformation steps followed by an estimator (the model). Using the pipeline is optional.\n",
    "\n",
    "Learning resources\n",
    "- [sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- [sklearn Transformers for text data](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)\n",
    "- [sklearn Feature Selection](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html?highlight=selectfrommodel#sklearn.feature_selection.SelectFromModel)\n",
    "- [sklearn Hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search)\n",
    "- [python packages for machine learning](https://github.com/ml-tooling/best-of-ml-python#time-series-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample code to build a preprocessor to transform input data. The numeric and categorical data are transformed using separate methods. In the sample code below there is a Pipline with a imputer (to fill in missing data) and a scaler to transform numerical data. Using the code below the categorical features are transformed into one hot encoded vector. Both these steps are combined to build the `preprocessor` module to perform input data transformation. \n",
    "\n",
    "Use the learning resources above to learn more about the individual parts of the `ColumnTransformer` module and to learn about the different options for data imputers, scalers, and text data transformers avialable \n",
    "\n",
    "\n",
    "```python3\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "    \n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "```\n",
    "\n",
    "`numeric_features` is a list of all numerical feature names. In our example `numerical_features=['acc', 'speed']` <br>\n",
    "`categorical_features` is a list of all numerical feature names\n",
    "\n",
    "\n",
    "To create and use relevant features you can use `SelecFromModel` for an additional `feature_selection` stage in the training pipeline\n",
    "\n",
    "```python3\n",
    "Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                 ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "                 ('classification', LinearRegression())])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['acc', 'speed'] # user-input-required\n",
    "\n",
    "# make sure we have these coulmns in data\n",
    "numeric_features = list(set(numeric_features).intersection(set(data_df.columns.to_list()))) # make sure we have these coulmns in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "## Example Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "skl_pipeline = Pipeline(steps=[('preprocessor', preprocessor), # (Optional) preprocessor stage\n",
    "                               ('classification', LinearRegression())]) # user-input-required estimator/machine learnign model you want to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.4 Train model**\n",
    "\n",
    "You have an option to run the training pipeline as set up in the cell above or to set up hyperparameter tuning in the cell below and run the search pipeline. The training pipeline runs a single training job while the hyperparameter search runs several jobs to find the best model parameters across the ranges specified in `param_grid`. The search pipeline is more computationally intensive and so it takes longer to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Set up hyper paratmeter tuning**\n",
    "\n",
    "- Parameters of pipelines can be set using `__` separated parameter names\n",
    "- specify parameters to train in `param_grid`\n",
    "- speciffy other parameters as needed\n",
    "\n",
    "Refer to [Tuning hyperparameter](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search) for further details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hyperparameter_tuning = True\n",
    "param_grid = {\n",
    "    'classification__learning_rate': 0.1 * (10 ** np.arange(0, 3))  \n",
    "}\n",
    "scoring = 'roc_auc'\n",
    "search = RandomizedSearchCV(skl_pipeline, param_grid, n_jobs=-1, scoring=scoring, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Training Job**\n",
    "\n",
    "Depending on factors like the model you picked, if you decided to run hyperparameter training, and the computational power you have to run this notebook, the cell below can take a few minutes to several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'use_hyperparameter_tuning' in globals() and use_hyperparameter_tuning:\n",
    "    print('Running the Hyperparameter tuning ...')\n",
    "    search.fit(X_train, y_train)\n",
    "    model = search\n",
    "else:\n",
    "    print('Running training pipeline ...')\n",
    "    model = skl_pipeline\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('Finished training job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run additional validation on model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "rmse = np.mean((np.round(y_pred) - y_test.values)**2)**0.5\n",
    "print('RMSE: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Save a local copy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "model_filename = 'monitor_wml_model.sav'\n",
    "pickle.dump(model, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Load the local copy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(model_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Deploy Model in WML\n",
    "\n",
    "In this step you will use the connection to wml python client made in **Step II. Connect to Watson ML API Client**. When deploying the trained mode you need to **a.** set the metadata, **b.** retain the model and, **c.** deploy the model to watson machine learning. \n",
    "\n",
    "For **a.** we will set the following metadata\n",
    "1. model name\n",
    "2. software speficication. Run the following command to get all software specificatons`client.software_specifications.list()`. Pick the specification best suited for your modeling\n",
    "3. hardware specification Run the following command to get all hardware specificatons`client.hardware_specifications.list()`. Pick the specification best suited for your modeling\n",
    "<br>\n",
    "(We extracted these specifications when we connected to the Watson Studio Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sw_spec_uid = client.software_specifications.get_uid_by_name(\"default_py3.7\") # user-input-required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up software specs when using only scikit-learn for modeling purposes. For more intricate sotware setup when using custom-packages refer to [Watson ML sample notebooks](https://www.ibm.com/docs/en/cloud-paks/cp-data/3.5.0?topic=apis-machine-learning-python-example-notebooks) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_props = {\n",
    "    client.repository.ModelMetaNames.NAME: \"MaxTemp prediction model\",\n",
    "    client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23', # user-input-required\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: base_sw_spec_uid\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Retain model**\n",
    "\n",
    "To save the trained model you need to specifiy a software specification.\n",
    "Learn more about available [Software Specicifications](https://dataplatform.cloud.ibm.com/docs/content/wsj/wmls/wmls-deploy-python-types.html?context=cpdaas&audience=wdp) and it's usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model = client.repository.store_model(model=model, meta_props=model_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Deploy model**\n",
    "\n",
    "For this deployment we used the hardware specification that requires the least amount of CUHs. The decision to use a different hardware specification can be guided by speed, performance and other variables <br>\n",
    "[More Information on CUH usage for different hardware specifications](https://dataplatform.cloud.ibm.com/docs/content/wsj/landings/wml-plans.html?context=cpdaas&audience=wdp)\n",
    "\n",
    "The default hardware specification is S(2CPUs 16GB). We are changing it to XXS(1CPU 8GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of test model\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "    client.deployments.ConfigurationMetaNames.HARDWARE_SPEC : { \"id\":  \"b128f957-581d-46d0-95b6-8af5cd5be580\"} # user-input-required\n",
    "}\n",
    "\n",
    "created_deployment = client.deployments.create(published_model_uid, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract deployment uuid and other information**\n",
    "\n",
    "We need the depolyment uuid to get predictions.\n",
    "We set this field in the **wml_auth** constant feild defined in [Monitor documentation](). This feild is used by the InvokeWatsonStudio catalog function to make real-time predictions using the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_uid = client.deployments.get_uid(created_deployment)\n",
    "print(deployment_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.get_details(deployment_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Information needed for scoring\n",
    "1. wml_credentials\n",
    "2. deployment space_id\n",
    "3. deployment id\n",
    "4. Feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) V. Test predictions using deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_endpoint = client.deployments.get_scoring_href(created_deployment)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_payload = {\n",
    "    \"input_data\": [{\n",
    "        'fields': ['acc', 'speed'],\n",
    "        'values': [[22, 23]]}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.deployments.score(deployment_uid, scoring_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have a trained model deployed in Watson Machine Learning services. Congratulations!\n",
    "\n",
    "Follow [Monitor documentation]() for catalog function **InvokeWatsonStudio** to see predictions within the Monitor pipeline using the deployed "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
