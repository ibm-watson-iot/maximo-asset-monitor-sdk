{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train** a ML model locally, **Deploy** the model in Monitor, and perform real-time **Prediction** in Maximo-Monitor\n",
    "\n",
    "You can use this notebook as template for training your model locally using data in Maximo-Monitor, and deploying the model in Monitor. At the end of this notebook there are further instructions on getting started with real-time inference.\n",
    "\n",
    "**Note** The following files are required to run this notebook and they are not provided with this template:\n",
    "1. monitor-credentials.json. Contains credentials to connect to maximo monitor. Contents explained in section **I. Credentials setup**\n",
    "\n",
    "*Create your own files with the correct file paths. In the template, all required inputs are marked with the `user-input-required` comment as a guide.*\n",
    "\n",
    "**Pre-requisites**\n",
    "1. Install iotfunctions package\n",
    "2. Install any other packages required to train your machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # loading credentials\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Credentials setup\n",
    "You must create one credentials file:\n",
    "1. Credentials to allow Maximo Monitor access its database and APIs.\n",
    "\n",
    "The credentials are saved in JSON files. In the following cell, define the relative path (that is, relative to this notebook) of both of these credentials files and load the files from local variables\n",
    "\n",
    "**Note**: Keep all credentials safe and hidden\n",
    "\n",
    "**More information about credentials in Maximo Monitor**\n",
    "\n",
    "[Maximo Monitor documentation](https://www.ibm.com/docs/en/maximo-monitor/8.5.0?topic=monitor-connection-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONITOR_CREDENTIALS_FILE_PATH = './dev_resources/monitor-credentials.json'  # user-input-required. Set it the path of your monitor-credentials file relative to this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MONITOR_CREDENTIALS_FILE_PATH, 'r') as f: \n",
    "    monitor_credentials = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Train Model\n",
    "\n",
    "[Read the article about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) if you are interested in getting familiar with some of the basics.\n",
    "\n",
    "This notebook can be modified and used for regression or classifiaction tasks. [A short refresher for classification vs regression tasks](https://in.springboard.com/blog/regression-vs-classification-in-machine-learning/)\n",
    "\n",
    "The template loads an entity's data and guides you to train a machine learning model locally using that data. An entity in Monitor has several database tables that store metrics, dimensions, derived metrics, and alerts. To load these features for an entity, follow this step-by-step guide. \n",
    "\n",
    "**Steps to train a model in a notebook**\n",
    "1. Load training data from Monitor database (for a specified entity)\n",
    "2. Prepare dataset for training\n",
    "3. Create a training pipeline\n",
    "4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1 Load training data from Monitor database**\n",
    "\n",
    "An entity in Monitor has several database tables that store raw metrics, dimensions, and alerts. To load these features you peform two database reads. The first read fetches raw metrics and dimensions, and the second read fetched the alerts for the user provided entity type. Both data streams are stored is separate comma separated value (.csv) files and are available to manipulate as required.\n",
    "\n",
    "\n",
    "Complete the following these steps to load training data:\n",
    "1. Specify entity_name (and any other user input)\n",
    "2. Connect to the database\n",
    "3. Save the data in a csv to use later\n",
    "\n",
    "**Note** If you already have data available in a csv file, go to section **III.2 Prepare data\n",
    "\n",
    "Set the following variables:\n",
    "- (Required) entity_name  # Name of the entity that you want to retrieve data from. Use the same name that is displayed on the user interface in Monitor.,\n",
    "- (Optional) start_ts # Start fetching date from this date and time.,\n",
    "- (Optional) end_ts # Fetch data until this date and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.1 Specify entity_name (and any other user input)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (User Input) user-input-required\n",
    "\n",
    "entity_name = 'shraddha_robot' # user-input-required. Set to entity name you'd like to retrieve data from. This is the same name as displayed on the Monitor UI\n",
    "\n",
    "start_ts = None # user-input-required. (Optional) Fetch data starting from this date/time. Format 'YYYY-MM-DD-HH.MM.ss.mmmmm'. Set to None to disable\n",
    "\n",
    "end_ts = None # user-input-required. (Optional) Fetch data until this date/time. Format 'YYYY-MM-DD-HH.MM.ss.mmmmm'. Set to None to disable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.2 Connect to the database**\n",
    "\n",
    "Use the monitor credentials loaded section **I. Credentials setup** to connect to the Monitor database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iotfunctions.db import Database\n",
    "\n",
    "db = Database(credentials = monitor_credentials, entity_type=entity_name)\n",
    "db_schema = None #  set if you are not using the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = db.get_entity_type(name=entity_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loaded data: metris and dimension**\n",
    "\n",
    "The metrics and dimension data for the entity type you specified is loaded in a dataframe that is indexed with a (id, evt_timestamp) index. `id` is the device ID and `evt_timestamp` is the event time. This is the same index scheme that the pipeline uses on its dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = entity_type.get_data(start_ts = start_ts, end_ts=end_ts) # fetch metric and dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Setup to fetch alerts)\n",
    "alert_table_name = \"DM_WIOT_AS_ALERT\"\n",
    "alert_table_timestamp_col = \"timestamp\"\n",
    "alert_filters = {\"entity_type_id\": [entity_type._entity_type_id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loaded data: alerts**\n",
    "\n",
    "Alert data for the specified entity type is loaded in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, _ = db.query(table_name=alert_table_name, schema=db_schema,timestamp_col=alert_table_timestamp_col, start_ts=start_ts, end_ts=end_ts, filters=alert_filters) #query for alert data\n",
    "alert_df = db.read_sql_query(sql=query.statement) # fetch alert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can perform further data evaluations and manipulations in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1.3 Save data in csv**\n",
    "\n",
    "The metric (with dimensions) and alert data that was loaded in the previous steps can now be saved into .csv files. Running the following step saves this data into files named **metric_{{entity_name}}.csv** and **alert_{{entity_name}}.csv** respectively. In your environment, `{{entity_name}}` is replaced by the name you specified during the load data stage.\n",
    "\n",
    "\n",
    "Both files are saved in the same directory as this notebook. You can create a separate directory for the data by appending the relative directory structure to the following file path. For example, to save the data to a folder named `data` under the directory that contains this notebook, modify the file path as follows:\n",
    "```python3\n",
    "metric_data_filepath = f'./data/metric_{entity_name}.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_data_filepath = f'metric_{entity_name}.csv'\n",
    "alert_data_filepath = f'alert_{entity_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.to_csv(metric_data_filepath, index=False,header=True) # save metric and dimension data\n",
    "alert_df.to_csv(alert_data_filepath, index=False,header=True) # save alert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.2 Prepare data**\n",
    "\n",
    "Load the data that you saved earlier and manipulates it some more to prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data back**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df = pd.read_csv(metric_data_filepath) # assumes you saved data using steps above. If you have pre-save data replace `metric_data_filepath` with the path to that .csv. The path should be relative to this notebook \n",
    "raw_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_data_df = pd.read_csv(alert_data_filepath) # assumes you saved data using steps above. If you have pre-save data replace `alert_data` with the path to that .csv. The path should be relative to this notebook \n",
    "alert_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataset to train the model**\n",
    "\n",
    "This template uses the variables `speed` and `acc` to predict `torque`. Depending on your problem statement, you can use all or part of the available data to define your feature vector as well as the prediction variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = ['acc', 'speed'] # use-input-required. Define feature vector. These are the variables used to train the model\n",
    "target_variable = 'torque' # use-input-required. Define target variable. This is the feature you want to predict. For regression models this will be a continous variable and for classification models this is a discrete variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_data_items = []\n",
    "required_data_items.extend(feature_vector)\n",
    "required_data_items.append(target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_data_items, feature_vector, target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = raw_data_df[required_data_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further data manipulation can be performed in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = data_df[target_variable]\n",
    "X = data_df[feature_vector]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=143) # (Optional) user-input-required. Change/add parameters to train_test_split function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cell for train, test data verification\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.3 Create training pipeline**\n",
    "\n",
    "In this template a sample training pipeline is provided. The training pipeline uses [sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that imlpements data transformation steps followed by an estimator (the model). Using the pipeline is optional.\n",
    "\n",
    "Learning resources\n",
    "- [Sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- [Sklearn Transformers for text data](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)\n",
    "- [Sklearn Feature Selection](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html?highlight=selectfrommodel#sklearn.feature_selection.SelectFromModel)\n",
    "- [Sklearn Hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search)\n",
    "- [Python packages for machine learning](https://github.com/ml-tooling/best-of-ml-python#time-series-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code builds a preprocessor to transform input data. The numeric and categorical data are transformed using separate methods. In the sample code, you'll find a pipeline with a imputer (to complete missing data) and a scaler to transform numerical data. Using the code, the categorical features are transformed into one hot encoded vector. Both of these steps are combined to build the `preprocessor` module to perform input data transformation.\n",
    "\n",
    "Use the sklearn learning resources to learn more about the individual parts of the `ColumnTransformer` module and to learn about the different options for data imputers, scalers, and text data transformers that are available.\n",
    "\n",
    "\n",
    "```python3\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "    \n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "```\n",
    "\n",
    "`numeric_features` is a list of all numerical feature names. In our example `numerical_features=['acc', 'speed']` <br>\n",
    "`categorical_features` is a list of all numerical feature names\n",
    "\n",
    "\n",
    "To create and use relevant features you can use `SelecFromModel` for an additional `feature_selection` stage in the training pipeline\n",
    "\n",
    "```python3\n",
    "Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                 ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "                 ('classification', LinearRegression())])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = list(set(X.columns.tolist()).difference(numeric_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "## Example Pipeline\n",
    "if numeric_features:\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "skl_pipeline = Pipeline(steps=[('preprocessor', preprocessor), # (Optional) preprocessor stage\n",
    "                               ('classification', GradientBoostingRegressor())]) # user-input-required estimator/machine learnign model you want to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.4 Train model**\n",
    "\n",
    "You can run the training pipeline by completing the setup in the previous cell or you can set up hyperparameter tuning as follows and run the search pipeline. The training pipeline runs a single training job while the hyperparameter search runs several jobs to find the best model parameters across the ranges specified in `param_grid`. The search pipeline is more computationally intensive and so it takes longer to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hyperparameter_tuning = False # (optional) user-input-required. Control for turning hyperparameter_tuning on or off. Run this cell to turn hyperparameter tuning off if you ran the cell below to turn it on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Set up hyper paratmeter tuning**\n",
    "\n",
    "- Parameters of pipelines can be set using `__` separated parameter names\n",
    "- specify parameters to train in `param_grid`\n",
    "- speciffy other parameters as needed\n",
    "\n",
    "Refer to [Tuning hyperparameter](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search) for further details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "use_hyperparameter_tuning = True \n",
    "param_grid = {\n",
    "    'classification__loss': ['ls', 'lad']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(skl_pipeline, param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Training Job**\n",
    "\n",
    "Depending on factors such as the model you selected, whether you are running hyperparameter training, and the computational power you have available to run this notebook, the following cell might take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'use_hyperparameter_tuning' in globals() and use_hyperparameter_tuning:\n",
    "    print('Running the Hyperparameter tuning ...')\n",
    "    search.fit(X_train, y_train)\n",
    "    model = search\n",
    "else:\n",
    "    print('Running training pipeline ...')\n",
    "    model = skl_pipeline\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('Finished training job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run additional validation on model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "rmse = np.mean((np.round(y_pred) - y_test.values)**2)**0.5\n",
    "print('RMSE: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Save a local copy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "model_filename = 'monitor_wml_model.sav'\n",
    "pickle.dump(model, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Load the local copy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(model_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Deploy Model in Monitor\n",
    "\n",
    "Use the connection to monitor database made in **III.1.2 Connect to the database**. When deploying the trained model store the feature vector in addition to the model. The feature vector is used for data validation within the custom-function used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and feature vector information is stored in monitor database and accessed by monitor's custom-function during inference time. The method shown here is one way to retain this information. When making a custiom-function you need **the keys to access the model and features** as well as the **model_name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model_name and feature vectors\n",
    "model_name = 'my_gbm_regressor' # user-input-required\n",
    "cache_model_and_features = { 'model': model,\n",
    "                             'features': feature_vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_model_and_features['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.model_store.store_model(model_name, cache_model_and_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) IV. Test predictions using deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model was saved correctly by accessing it\n",
    "deployed_model_metadata = db.model_store.retrieve_model(model_name)\n",
    "deployed_model_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate the model and features**\n",
    "\n",
    "Use the feature vector to perform data validation. A basic data validation is to make sure that all the specified features are present and in correct order in the dataframe used for prediction. \n",
    "\n",
    "Use the model to generate prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = deployed_model_metadata['model']\n",
    "deployed_features = deployed_model_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test prediction**\n",
    "\n",
    "In this sample the test split from earlier is used to validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = deployed_model.predict(X_test)\n",
    "rmse = np.mean((np.round(y_pred) - y_test.values)**2)**0.5\n",
    "print('RMSE: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Sample code to delete any old model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.model_store.delete_model(model_name=\"my_gbm_regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, you have deployed a trained model in Monitor Database. Congratulations!\n",
    "\n",
    "### Setup for real-time inference\n",
    "The last part of this machine learning experience is to use the deployed model for inference. For this step you need to [create a custom function](https://github.com/ibm-watson-iot/functions/tree/advanced_custom_function_starter). [Example custom function used for inference](https://github.com/singhshraddha/custom-functions/blob/development/custom/forecast.py#L156)\n",
    "\n",
    "Within the `execute` method of the custom-function you will\n",
    "1. retrive the model and feature vector from the monitor database\n",
    "2. perform data validation using the feature vector\n",
    "3. call the prediction method for the estimator you used\n",
    "\n",
    "The custom-function should have a way to accept model name parameter and the feature names as parameter. This will be done in the `build_ui` and `__init__` methods. To select features regardless of datatype \n",
    "[set the corresponding UI item's datatype=None](https://github.com/singhshraddha/custom-functions/blob/1923713d7cfc71c8a0fb9f4bc9da365a40bc733e/custom/forecast.py#L156) *in this case UIMultiItem*. This setting lets you select features name across all metrics, dimensions, and alerts. \n",
    "\n",
    "After creating and registering a cutom function, follow these steps to get the predictions on the UI <br>\n",
    "**From the Monitor UI setup assests data tab** <br>\n",
    "1. Choose your custom function from the catalog <br>\n",
    "2. Set saved_model_name parameter to model_name <br>\n",
    "3. Select raw metric/dimension/alert features used for trainign the model as the features parameter from the drop down <br>\n",
    "4. Specify the target/output name <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
